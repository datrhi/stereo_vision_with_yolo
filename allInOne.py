# -*- coding: utf-8 -*-

# Form implementation generated from reading ui file 'new-draft.ui'
#
# Created by: PyQt5 UI code generator 5.15.6
#
# WARNING: Any manual changes made to this file will be lost when pyuic5 is
# run again.  Do not edit this file unless you know what you are doing.


import numpy as np
import cv2
import time
from PyQt5 import QtCore, QtGui, QtWidgets
from PyQt5.QtGui import *
from PyQt5.QtWidgets import *
from PyQt5.QtCore import *
from threading import Thread
start = True
dataset = None
cfg = None
weight = None
video_source_index = None
# Constant
base_line = 120
focal_length = 3
mapMMToPixel = 375
listPort = []
constant = base_line * focal_length * mapMMToPixel * 0.48
FRAME_WIDTH = 1280
FRAME_HEIGHT = 480


def draw_text(img, text,
              font=cv2.FONT_HERSHEY_PLAIN,
              pos=(0, 0),
              font_scale=3,
              font_thickness=2,
              text_color=(0, 255, 0),
              text_color_bg=(0, 0, 0)
              ):

    x, y = pos
    text_size, _ = cv2.getTextSize(text, font, font_scale, font_thickness)
    text_w, text_h = text_size
    cv2.rectangle(img, pos, (x + text_w, y + text_h), text_color_bg, -1)
    cv2.putText(img, text, (x, y + text_h + font_scale - 1),
                font, font_scale, text_color, font_thickness)

    return text_size


class SpecialBox:
    def __init__(self, x_center, y_center, area, classIDs, box_width, box_height):
        self.x_center = x_center
        self.y_center = y_center
        self.area = area
        self.classIDs = classIDs
        # for cal width, height
        self.box_width = box_width
        self.box_height = box_height


class Depth:
    def __init__(self, depth, x_min, y_min, box_width, box_height, classIDs):
        self.depth = depth
        self.x_min = x_min
        self.y_min = y_min
        self.classIDs = classIDs
        self.box_width = box_width
        self.box_height = box_height

    def __str__(self):
        return '(\nDepth: {},\nX: {},\nY: {},\nclassIDs: {}\n)\n'.format(self.depth, self.x_min, self.y_min, self.classIDs)

    def __hash__(self):
        return hash(('classIDs', self.classIDs))


class Worker1(QThread):
    ImageUpdate = pyqtSignal(QImage)

    def __init__(self, cameraIndex, **kwargs):
        self.cameraIndex = cameraIndex
        super(Worker1, self).__init__(**kwargs)

    def run(self):
        global video_source_index, dataset, cfg, weight
        self.ThreadActive = True
        self.camera = cv2.VideoCapture(video_source_index)

        self.camera.set(cv2.CAP_PROP_FRAME_WIDTH, FRAME_WIDTH)
        self.camera.set(cv2.CAP_PROP_FRAME_HEIGHT, FRAME_HEIGHT)
        h, w = None, None
        with open(dataset) as f:
            # Getting labels reading every line
            # and putting them into the list
            labels = [line.strip() for line in f]

            # print('List with labels names:')
            # print(labels)

        # Loading trained YOLO Objects Detector with the help of 'dnn' library from OpenCV
        network = cv2.dnn.readNetFromDarknet(cfg,
                                             weight)

        # Getting list with names of all layers from YOLO v3 network
        layers_names_all = network.getLayerNames()

        # print(layers_names_all)

        # Getting only output layers' names that we need from YOLO v3 algorithm
        # with function that returns indexes of layers with unconnected outputs
        layers_names_output = \
            [layers_names_all[i - 1]
                for i in network.getUnconnectedOutLayers()]

        # print()
        # print(layers_names_output)  # ['yolo_82', 'yolo_94', 'yolo_106']

        # Set min prob to eliminate weak prediction
        probability_minimum = 0.5

        # Setting threshold for filtering weak bounding boxes with non-maximum suppression
        threshold = 0.3

        # Generating colours for representing every detected object
        # with function randint(low, high=None, size=None, dtype='l')
        colours = np.random.randint(
            0, 255, size=(len(labels), 3), dtype='uint8')
        while self.ThreadActive:
            # Capturing frame-by-frame from self.camera
            _, frame = self.camera.read()

            # frame = cv2.remap(frameL,Left_Stereo_Map[0],Left_Stereo_Map[1], cv2.INTER_LANCZOS4, cv2.BORDER_CONSTANT, 0)
            # Getting spatial dimensions of the frame we do it only once from the very beginning
            # all other frames have the same dimension
            if w is None or h is None:
                # Slicing from tuple only first two elements
                h, w = frame.shape[:2]

            # Getting blob from current frame using method blobFromImage provided by opencv
            # The 'cv2.dnn.blobFromImage' function returns 4-dimensional blob from current
            # frame after mean subtraction, normalizing, and RB channels swapping
            # Resulted shape has number of frames, number of channels, width and height
            # eg.:
            # blob = cv2.dnn.blobFromImage(image, scalefactor=1.0, size, mean, swapRB=True)
            blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),
                                         swapRB=True, crop=False)

            # Implementing forward pass with our blob and only through output layers
            # Calculating at the same time, needed time for forward pass
            network.setInput(blob)  # setting blob as input to the network
            start = time.time()
            output_from_network = network.forward(layers_names_output)
            

            # Preparing lists for detected bounding boxes, obtained confidences and class's number
            bounding_boxes = []
            confidences = []
            classIDs = []

            # Y_CENTER_ARR = []
            specBox = []
            # Going through all output layers after feed forward pass
            for result in output_from_network:
                # Going through all detections from current output layer
                for detected_objects in result:
                    # Getting 80 classes' probabilities for current detected object
                    scores = detected_objects[5:]
                    # Getting index of the class with the maximum value of probability
                    class_current = np.argmax(scores)
                    # Getting value of probability for defined class
                    confidence_current = scores[class_current]

                    # # Every 'detected_objects' numpy array has first 4 numbers with
                    # # bounding box coordinates and rest 80 with probabilities
                    # # for every class
                    # print(detected_objects.shape)  # (85,)

                    # Eliminating weak predictions with minimum probability
                    if confidence_current > probability_minimum:
                        # Scaling bounding box coordinates to the initial frame size
                        # YOLO data format keeps coordinates for center of bounding box
                        # and its current width and height
                        # That is why we can just multiply them elementwise
                        # to the width and height
                        # of the original frame and in this way get coordinates for center
                        # of bounding box, its width and height for original frame
                        box_current = detected_objects[0:4] * \
                            np.array([w, h, w, h])

                        # Now, from YOLO data format, we can get top left corner coordinates that are x_min and y_min
                        x_center, y_center, box_width, box_height = box_current
                        x_min = int(x_center - (box_width / 2))
                        y_min = int(y_center - (box_height / 2))

                        bounding_boxes.append(
                            [x_min, y_min, int(box_width), int(box_height)])

                        confidences.append(float(confidence_current))
                        classIDs.append(class_current)

            # Implementing non-maximum suppression of given bounding boxes
            # With this technique we exclude some of bounding boxes if their
            # corresponding confidences are low or there is another
            # bounding box for this region with higher confidence

            # It is needed to make sure that data type of the boxes is 'int'
            # and data type of the confidences is 'float'
            # https://github.com/opencv/opencv/issues/12789
            results = cv2.dnn.NMSBoxes(bounding_boxes, confidences,
                                       probability_minimum, threshold)
            if len(results) > 0:
                for i in results.flatten():
                    specBox.append(SpecialBox(
                        bounding_boxes[i][0] + (bounding_boxes[i][2] / 2), bounding_boxes[i][1] + (bounding_boxes[i][3] / 2), bounding_boxes[i][2] * bounding_boxes[i][3], int(classIDs[i]), bounding_boxes[i][2], bounding_boxes[i][3]))

            # Sá»‘ bounding box
            cv2.putText(frame, '{}'.format(len(specBox)), (int(w/2), int(h)),
                        cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 2, 2)
            specBox.sort(key=lambda x: (x.y_center, x.x_center))
            coupleBox = []
            for i, box in enumerate(specBox):
                id = i + 1
                while id < len(specBox):
                    if (specBox[id].y_center - box.y_center) < 40 and specBox[id].classIDs == box.classIDs \
                            and ((specBox[id].x_center >= int(FRAME_WIDTH/2) and box.x_center <= int(FRAME_WIDTH/2)) or (specBox[id].x_center <= int(FRAME_WIDTH/2) and box.x_center >= int(FRAME_WIDTH/2))):
                        if specBox[id].x_center >= int(FRAME_WIDTH/2):
                            disparity = box.x_center - \
                                (specBox[id].x_center - int(FRAME_WIDTH/2))
                            if disparity == 0:
                              coupleBox.append(
                                  Depth(-1, int(box.x_center - (box.box_width / 2)), int(box.y_center - (box.box_height / 2)),
                                        box.box_width, box.box_height,
                                        box.classIDs))
                            else:
                              depth = (constant) / (disparity*1000)
                              coupleBox.append(
                                  Depth(depth, int(box.x_center - (box.box_width / 2)), int(box.y_center - (box.box_height / 2)),
                                        box.box_width, box.box_height,
                                        box.classIDs))
                        else:
                            disparity = specBox[id].x_center - \
                                (box.x_center - int(FRAME_WIDTH/2))
                            if disparity == 0:
                              coupleBox.append(
                                  Depth(-1, int(box.x_center - (box.box_width / 2)), int(box.y_center - (box.box_height / 2)),
                                        box.box_width, box.box_height,
                                        box.classIDs))
                            else:
                              depth = (constant) / (disparity*1000)
                              coupleBox.append(
                                  Depth(depth, int(box.x_center - (box.box_width / 2)), int(box.y_center - (box.box_height / 2)),
                                        box.box_width, box.box_height,
                                        box.classIDs))
                        break
                    elif specBox[id].y_center - box.y_center >= 10:
                        break
                    id = id + 1
            coupleBox = list(set(coupleBox))
            end = time.time()

            # Showing spent time for single current frame
            print('Current frame took {:.5f} seconds'.format(end - start))
            if len(coupleBox) > 0:
                for i, box in enumerate(coupleBox):
                    if (box.depth == -1):
                      draw_text(frame, '{}: Too far'.format(
                        labels[int(box.classIDs)]), font_scale=2 , pos=(box.x_min, box.y_min))
                    else:
                      draw_text(frame, '{}: {:.2f} m'.format(
                        labels[int(box.classIDs)], abs(box.depth)), font_scale=2 , pos=(box.x_min, box.y_min))
                    # boxWidth = (box.box_width * abs(box.depth) * 1.7)/(focal_length*mapMMToPixel)
                    # boxHeight = (box.box_height * abs(box.depth) * 1.7) / \
                    #     (focal_length*mapMMToPixel)

                    # cv2.putText(frame, 'W: {:.2f} m'.format(boxWidth), (int(box.x_min + (box.box_width / 2)), box.y_min + box.box_height),
                    #             cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
                    # cv2.putText(frame, 'H: {:.2f} m'.format(boxHeight), (box.x_min + box.box_width, int(box.y_min + (box.box_height / 2))),
                    #             cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

            # Non-maximum suppression

            # Checking if there is at least one detected object
            # after non-maximum suppression
            if len(results) > 0:
                # Going through indexes of results
                for i in results.flatten():
                    # Getting current bounding box coordinates, its width and height
                    x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]
                    box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]

                    # Preparing colour for current bounding box and converting from numpy array to list
                    colour_box_current = colours[classIDs[i]].tolist()

                    # Drawing bounding box on the original current frame
                    cv2.rectangle(frame, (x_min, y_min),
                                  (x_min + box_width, y_min + box_height),
                                  colour_box_current, 2)

                    # Preparing text with label and confidence for current bounding box
                    text_box_current = '{}: {:.4f}'.format(labels[int(classIDs[i])],
                                                           confidences[i])

                    # Putting text with label and confidence on the original image
                    cv2.putText(frame, text_box_current, (x_min, y_min - 5),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, colour_box_current, 2)
                    # cv2.putText(frame, 'disparity:{}'.format(disparity[i]), (x_min, y_min - 20),
                    #             cv2.FONT_HERSHEY_SIMPLEX, 0.5, colour_box_current, 2)
                    # cv2.putText(frame, 'area:{}'.format(box_width*box_height), (x_min, y_min - 40),
                    #             cv2.FONT_HERSHEY_SIMPLEX, 0.5, colour_box_current, 2)
            Image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            # FlippedImage = cv2.flip(Image, 1)
            ConvertToQtFormat = QImage(
                Image.data, Image.shape[1], Image.shape[0], QImage.Format_RGB888)
            Pic = ConvertToQtFormat.scaled(
                FRAME_WIDTH, FRAME_HEIGHT, Qt.KeepAspectRatio)
            self.ImageUpdate.emit(Pic)
    def stop(self):
        self.ThreadActive = False
        self.camera.release()
        self.quit()


class Ui_MainWindow(object):
    def setupUi(self, MainWindow):
        MainWindow.setObjectName("MainWindow")
        MainWindow.resize(1450, 872)
        MainWindow.setMaximumSize(QtCore.QSize(1920, 1080))
        self.centralwidget = QtWidgets.QWidget(MainWindow)
        self.centralwidget.setMaximumSize(QtCore.QSize(1920, 1080))
        self.centralwidget.setObjectName("centralwidget")
        self.pushButton = QtWidgets.QPushButton(self.centralwidget)
        self.pushButton.setGeometry(QtCore.QRect(520, 660, 371, 151))
        self.pushButton.setObjectName("pushButton")
        self.pushButton_2 = QtWidgets.QPushButton(self.centralwidget)
        self.pushButton_2.setGeometry(QtCore.QRect(920, 660, 91, 41))
        self.pushButton_2.setObjectName("pushButton_2")
        self.pushButton_4 = QtWidgets.QPushButton(self.centralwidget)
        self.pushButton_4.setGeometry(QtCore.QRect(920, 770, 91, 41))
        self.pushButton_4.setObjectName("pushButton_4")
        self.pushButton_9 = QtWidgets.QPushButton(self.centralwidget)
        self.pushButton_9.setGeometry(QtCore.QRect(80, 660, 141, 51))
        self.pushButton_9.setObjectName("pushButton_9")
        self.label = QtWidgets.QLabel(self.centralwidget)
        self.label.setGeometry(QtCore.QRect(230, 660, 261, 51))
        self.label.setText("")
        self.label.setObjectName("label")
        self.pushButton_10 = QtWidgets.QPushButton(self.centralwidget)
        self.pushButton_10.setGeometry(QtCore.QRect(80, 710, 141, 51))
        self.pushButton_10.setObjectName("pushButton_10")
        self.label_2 = QtWidgets.QLabel(self.centralwidget)
        self.label_2.setGeometry(QtCore.QRect(230, 710, 261, 51))
        self.label_2.setText("")
        self.label_2.setObjectName("label_2")
        self.pushButton_11 = QtWidgets.QPushButton(self.centralwidget)
        self.pushButton_11.setGeometry(QtCore.QRect(80, 760, 141, 51))
        self.pushButton_11.setObjectName("pushButton_11")
        self.label_3 = QtWidgets.QLabel(self.centralwidget)
        self.label_3.setGeometry(QtCore.QRect(230, 760, 261, 51))
        self.label_3.setText("")
        self.label_3.setObjectName("label_3")
        self.pushButton_3 = QtWidgets.QPushButton(self.centralwidget)
        self.pushButton_3.setGeometry(QtCore.QRect(1040, 730, 231, 81))
        self.pushButton_3.setObjectName("pushButton_3")
        self.comboBox = QtWidgets.QComboBox(self.centralwidget)
        self.comboBox.setGeometry(QtCore.QRect(1040, 690, 231, 31))
        self.comboBox.setObjectName("comboBox")
        self.label_4 = QtWidgets.QLabel(self.centralwidget)
        self.label_4.setGeometry(QtCore.QRect(1040, 660, 201, 21))
        self.label_4.setStyleSheet("font: 12pt \"MS Shell Dlg 2\";")
        self.label_4.setObjectName("label_4")
        self.FeedLabel = QtWidgets.QLabel(self.centralwidget)
        self.FeedLabel.setGeometry(QtCore.QRect(100, 20, 1280, 480))
        # self.FeedLabel.setText("")
        self.FeedLabel.setObjectName("FeedLabel")
        MainWindow.setCentralWidget(self.centralwidget)
        self.menubar = QtWidgets.QMenuBar(MainWindow)
        self.menubar.setGeometry(QtCore.QRect(0, 0, 1450, 21))
        self.menubar.setObjectName("menubar")
        MainWindow.setMenuBar(self.menubar)
        self.statusbar = QtWidgets.QStatusBar(MainWindow)
        self.statusbar.setObjectName("statusbar")
        MainWindow.setStatusBar(self.statusbar)

        self.retranslateUi(MainWindow)
        QtCore.QMetaObject.connectSlotsByName(MainWindow)

    def retranslateUi(self, MainWindow):
        global listPort
        _translate = QtCore.QCoreApplication.translate
        MainWindow.setWindowTitle(_translate(
            "MainWindow", "Depth Estimation"))
        self.pushButton.setText(_translate("MainWindow", "Start"))
        self.pushButton_2.setText(_translate("MainWindow", "Stable mode"))
        self.pushButton_4.setText(_translate("MainWindow", "Debug mode"))
        self.pushButton_9.setText(_translate(
            "MainWindow", "Choose dataset file"))
        self.pushButton_10.setText(
            _translate("MainWindow", "Choose cfg file"))
        self.pushButton_11.setText(_translate(
            "MainWindow", "Choose weight file"))
        self.pushButton_3.setText(_translate("MainWindow", "Calibrate"))
        self.label_4.setText(_translate(
            "MainWindow", "Video source index"))
        self.pushButton_9.clicked.connect(self.uploadDataset)
        self.pushButton_10.clicked.connect(self.uploadCfg)
        self.pushButton_11.clicked.connect(self.uploadWeight)
        self.pushButton.clicked.connect(self.btn_thread)
        _, listPort = self.list_ports()
        self.comboBox.addItems([str(port) for port in listPort])
        self.comboBox.currentTextChanged.connect(
            self.video_source_on_change)
        self.getConfig()

    def ImageUpdateSlot(self, Image):
        self.FeedLabel.setPixmap(QPixmap.fromImage(Image))
    def getConfig(self):
        global dataset, cfg, weight
        try:
          file1 = open("config.txt","r")
          cfgs = file1.readlines()
          listConfig = [cfg[:len(cfg)-1] for cfg in cfgs]
          if len(listConfig) == 3:
            dataset = listConfig[0]
            cfg = listConfig[1]
            weight = listConfig[2]
            self.label.setText(dataset)
            self.label_2.setText(cfg)
            self.label_3.setText(weight)
          else: 
            msg = QtWidgets.QMessageBox()
            msg.setWindowTitle("Warning")
            msg.setIcon(QtWidgets.QMessageBox.Warning)
            msg.setText("Default configuration is missing some paths, please add them manually to run program!")
            msg.exec_()
        except:
          msg = QtWidgets.QMessageBox()
          msg.setWindowTitle("Warning")
          msg.setIcon(QtWidgets.QMessageBox.Warning)
          msg.setText("Can not open file config.txt. Make sure you did not delete them!")
          msg.exec_()
    def list_ports(self):
        is_working = True
        dev_port = 0
        working_ports = []
        available_ports = []
        while is_working:
            camera = cv2.VideoCapture(dev_port)
            if not camera.isOpened():
                is_working = False
                # print("Port %s is not working." % dev_port)
            else:
                is_reading, img = camera.read()
                w = camera.get(3)
                h = camera.get(4)
                if is_reading:
                    # print("Port %s is working and reads images (%s x %s)" %(dev_port,h,w))
                    working_ports.append(dev_port)
                else:
                    # print("Port %s for camera ( %s x %s) is present but does not reads." %(dev_port,h,w))
                    available_ports.append(dev_port)
            dev_port += 1
        return available_ports, working_ports

    def video_source_on_change(self, current):
        global video_source_index
        if current is not None:
            video_source_index = int(current)

    def btn_thread(self):
        global start, dataset, cfg, weight, video_source_index, listPort
        if dataset is not None and cfg is not None and weight is not None and len(listPort) > 0:
            if video_source_index is None:
                video_source_index = listPort[0]
            if start:
                self.pushButton.setText("Stop")
                start = False
                self.Worker1 = Worker1(video_source_index)

                self.Worker1.start()
                self.Worker1.ImageUpdate.connect(self.ImageUpdateSlot)
            else:
                self.pushButton.setText("Start")
                start = True
                self.Worker1.stop()

        else:
            msg = QtWidgets.QMessageBox()
            msg.setWindowTitle("Error")
            msg.setIcon(QtWidgets.QMessageBox.Critical)
            msg.setText("Please set up config first!!")
            msg.exec_()

    def uploadDataset(self):
        global dataset
        dataset, _ = QtWidgets.QFileDialog.getOpenFileName(
            None, "Open File", "", "All Files (*)")
        if dataset:
            self.label.setText(dataset)
            file1 = open("config.txt","a")#append mode
            file1.write("{}\n".format(dataset))

    def uploadCfg(self):
        global cfg
        cfg, _ = QtWidgets.QFileDialog.getOpenFileName(
            None, "Open File", "", "All Files (*)")
        if cfg:
            self.label_2.setText(cfg)
            file1 = open("config.txt","a")#append mode
            file1.write("{}\n".format(cfg))

    def uploadWeight(self):
        global weight
        weight, _ = QtWidgets.QFileDialog.getOpenFileName(
            None, "Open File", "", "All Files (*)")
        if weight:
            self.label_3.setText(weight)
            file1 = open("config.txt","a")#append mode
            file1.write("{}\n".format(weight))


if __name__ == "__main__":
    import sys
    app = QtWidgets.QApplication(sys.argv)
    MainWindow = QtWidgets.QMainWindow()
    ui = Ui_MainWindow()
    ui.setupUi(MainWindow)
    MainWindow.show()
    sys.exit(app.exec_())
